About DSPy | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub About DSPy Quick Start DSPy Building Blocks Tutorials FAQs DSPy Cheatsheet Deep Dive internal About DSPy On this page About DSPy DSPy is a framework for algorithmically optimizing LM prompts and weights , especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system without DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate synthetic examples to tune each step, and (5) use these examples to finetune smaller LMs to cut costs. Currently, this is hard and messy: every time you change your pipeline, your LM, or your data, all prompts (or finetuning steps) may need to change. To make this more systematic and much more powerful, DSPy does two things. First, it separates the flow of your program ( modules ) from the parameters (LM prompts and weights) of each step. Second, DSPy introduces new optimizers , which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a metric you want to maximize. DSPy can routinely teach powerful models like GPT-3.5 or GPT-4 and local models like T5-base or Llama2-13b to be much more reliable at tasks, i.e. having higher quality and/or avoiding specific failure patterns. DSPy optimizers will "compile" the same program into different instructions, few-shot prompts, and/or weight updates (finetunes) for each LM. This is a new paradigm in which LMs and their prompts fade into the background as optimizable pieces of a larger system that can learn from data. tldr; less prompting, higher scores, and a more systematic approach to solving hard tasks with LMs. Analogy to Neural Networks ‚Äã When we build neural networks, we don't write manual for-loops over lists of hand-tuned floats. Instead, you might use a framework like PyTorch to compose layers (e.g., Convolution or Dropout ) and then use optimizers (e.g., SGD or Adam) to learn the parameters of the network. Ditto! DSPy gives you the right general-purpose modules (e.g., ChainOfThought , ReAct , etc.), which replace string-based prompting tricks. To replace prompt hacking and one-off synthetic data generators, DSPy also gives you general optimizers ( BootstrapFewShotWithRandomSearch or MIPRO ), which are algorithms that update parameters in your program. Whenever you modify your code, your data, your assertions, or your metric, you can compile your program again and DSPy will create new effective prompts that fit your changes. Next Quick Start Analogy to Neural Networks Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è About DSPy | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub About DSPy Quick Start DSPy Building Blocks Tutorials FAQs DSPy Cheatsheet Deep Dive internal About DSPy On this page About DSPy DSPy is a framework for algorithmically optimizing LM prompts and weights , especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system without DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate synthetic examples to tune each step, and (5) use these examples to finetune smaller LMs to cut costs. Currently, this is hard and messy: every time you change your pipeline, your LM, or your data, all prompts (or finetuning steps) may need to change. To make this more systematic and much more powerful, DSPy does two things. First, it separates the flow of your program ( modules ) from the parameters (LM prompts and weights) of each step. Second, DSPy introduces new optimizers , which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a metric you want to maximize. DSPy can routinely teach powerful models like GPT-3.5 or GPT-4 and local models like T5-base or Llama2-13b to be much more reliable at tasks, i.e. having higher quality and/or avoiding specific failure patterns. DSPy optimizers will "compile" the same program into different instructions, few-shot prompts, and/or weight updates (finetunes) for each LM. This is a new paradigm in which LMs and their prompts fade into the background as optimizable pieces of a larger system that can learn from data. tldr; less prompting, higher scores, and a more systematic approach to solving hard tasks with LMs. Analogy to Neural Networks ‚Äã When we build neural networks, we don't write manual for-loops over lists of hand-tuned floats. Instead, you might use a framework like PyTorch to compose layers (e.g., Convolution or Dropout ) and then use optimizers (e.g., SGD or Adam) to learn the parameters of the network. Ditto! DSPy gives you the right general-purpose modules (e.g., ChainOfThought , ReAct , etc.), which replace string-based prompting tricks. To replace prompt hacking and one-off synthetic data generators, DSPy also gives you general optimizers ( BootstrapFewShotWithRandomSearch or MIPRO ), which are algorithms that update parameters in your program. Whenever you modify your code, your data, your assertions, or your metric, you can compile your program again and DSPy will create new effective prompts that fit your changes. Next Quick Start Analogy to Neural Networks Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è DSPy Documentation | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub DSPy Programming‚Äînot prompting‚ÄîLanguage Models Get Started with DSPy The Way of DSPy Systematic Optimization Choose from a range of optimizers to enhance your program. Whether it's generating refined instructions, or fine-tuning weights, DSPy's optimizers are engineered to maximize efficiency and effectiveness. Modular Approach With DSPy, you can build your system using predefined modules, replacing intricate prompting techniques with straightforward, effective solutions. Cross-LM Compatibility Whether you're working with powerhouse models like GPT-3.5 or GPT-4, or local models such as T5-base or Llama2-13b, DSPy seamlessly integrates and enhances their performance in your system. Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è Tutorials | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub About DSPy Quick Start DSPy Building Blocks Tutorials [01] RAG: Retrieval-Augmented Generation [02] Multi-Hop Question Answering Community Examples Additional Resources FAQs DSPy Cheatsheet Deep Dive internal Tutorials Tutorials Step-by-step illustrations of solving a task in DSPy. üìÑÔ∏è [01] RAG: Retrieval-Augmented Generation Retrieval-augmented generation (RAG) is an approach that allows LLMs to tap into a large corpus of knowledge from sources and query its knowledge store to find relevant passages/content and produce a well-refined response. üìÑÔ∏è [02] Multi-Hop Question Answering A single search query is often not enough for complex QA tasks. For instance, an example within HotPotQA includes a question about the birth city of the writer of "Right Back At It Again". A search query often identifies the author correctly as "Jeremy McKinnon", but lacks the capability to compose the intended answer in determining when he was born. üìÑÔ∏è Community Examples The DSPy team believes complexity has to be justified. We take this seriously: we never release a complex tutorial (above) or example (below) unless we can demonstrate empirically that this complexity has generally led to improved quality or cost. This kind of rule is rarely enforced by other frameworks or docs, but you can count on it in DSPy examples. üìÑÔ∏è Additional Resources Tutorials Previous Typed Predictors Next [01] RAG: Retrieval-Augmented Generation Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è API References | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub API References Modules Functional Optimizers Retrieval Model Clients Language Model API Clients Local Language Model Clients DSPy Assertions API References API References Welcome to the API References for DSPy! This is where you'll find easy-to-understand information about all the parts of DSPy that you can use in your projects. We've got guides on different tools and helpers that DSPy has, like modules and optimizers. Everything is sorted so you can quickly find what you need. If you're making something and need to quickly get started with DSPy to do certain tasks, this place will show you how to set it up and get it working just right. Next Modules Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è DSPy Cheatsheet | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub About DSPy Quick Start DSPy Building Blocks Tutorials FAQs DSPy Cheatsheet Deep Dive internal DSPy Cheatsheet On this page DSPy Cheatsheet This page will contain snippets for frequent usage patterns. DSPy DataLoaders ‚Äã Import and initializing a DataLoader Object: import dspy from dspy . datasets import DataLoader dl = DataLoader ( ) Loading from HuggingFace Datasets ‚Äã code_alpaca = dl . from_huggingface ( "HuggingFaceH4/CodeAlpaca_20K" ) You can access the dataset of the splits by calling key of the corresponding split: train_dataset = code_alpaca [ 'train' ] test_dataset = code_alpaca [ 'test' ] Loading specific splits from HuggingFace ‚Äã You can also manually specify splits you want to include as a parameters and it'll return a dictionary where keys are splits that you specified: code_alpaca = dl . from_huggingface ( "HuggingFaceH4/CodeAlpaca_20K" , split = [ "train" , "test" ] , ) print ( f"Splits in dataset: { code_alpaca . keys ( ) } " ) If you specify a single split then dataloader will return a List of dspy.Example instead of dictionary: code_alpaca = dl . from_huggingface ( "HuggingFaceH4/CodeAlpaca_20K" , split = "train" , ) print ( f"Number of examples in split: { len ( code_alpaca ) } " ) You can slice the split just like you do with HuggingFace Dataset too: code_alpaca_80 = dl . from_huggingface ( "HuggingFaceH4/CodeAlpaca_20K" , split = "train[:80%]" , ) print ( f"Number of examples in split: { len ( code_alpaca_80 ) } " ) code_alpaca_20_80 = dl . from_huggingface ( "HuggingFaceH4/CodeAlpaca_20K" , split = "train[20%:80%]" , ) print ( f"Number of examples in split: { len ( code_alpaca_20_80 ) } " ) Loading specific subset from HuggingFace ‚Äã If a dataset has a subset you can pass it as an arg like you do with load_dataset in HuggingFace: gms8k = dl . from_huggingface ( "gsm8k" , "main" , input_keys = ( "question" , ) , ) print ( f"Keys present in the returned dict: { list ( gms8k . keys ( ) ) } " ) print ( f"Number of examples in train set: { len ( gms8k [ 'train' ] ) } " ) print ( f"Number of examples in test set: { len ( gms8k [ 'test' ] ) } " ) Loading from CSV ‚Äã dolly_100_dataset = dl . from_csv ( "dolly_subset_100_rows.csv" , ) You can choose only selected columns from the csv by specifying them in the arguments: dolly_100_dataset = dl . from_csv ( "dolly_subset_100_rows.csv" , fields = ( "instruction" , "context" , "response" ) , input_keys = ( "instruction" , "context" ) ) Splitting a List of dspy.Example ‚Äã splits = dl . train_test_split ( dataset , train_size = 0.8 ) # `dataset` is a List of dspy.Example train_dataset = splits [ 'train' ] test_dataset = splits [ 'test' ] Sampling from List of dspy.Example ‚Äã sampled_example = dl . sample ( dataset , n = 5 ) # `dataset` is a List of dspy.Example DSPy Programs ‚Äã dspy.Signature ‚Äã class BasicQA ( dspy . Signature ) : """Answer questions with short factoid answers.""" question = dspy . InputField ( ) answer = dspy . OutputField ( desc = "often between 1 and 5 words" ) dspy.ChainOfThought ‚Äã generate_answer = dspy . ChainOfThought ( BasicQA ) # Call the predictor on a particular input alongside a hint. question = 'What is the color of the sky?' pred = generate_answer ( question = question ) dspy.ChainOfThoughtwithHint ‚Äã generate_answer = dspy . ChainOfThoughtWithHint ( BasicQA ) # Call the predictor on a particular input alongside a hint. question = 'What is the color of the sky?' hint = "It's what you often see during a sunny day." pred = generate_answer ( question = question , hint = hint ) dspy.ProgramOfThought ‚Äã pot = dspy . ProgramOfThought ( BasicQA ) question = 'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?' result = pot ( question = question ) print ( f"Question: { question } " ) print ( f"Final Predicted Answer (after ProgramOfThought process): { result . answer } " ) dspy.ReACT ‚Äã react_module = dspy . ReAct ( BasicQA ) question = 'Sarah has 5 apples. She buys 7 more apples from the store. How many apples does Sarah have now?' result = react_module ( question = question ) print ( f"Question: { question } " ) print ( f"Final Predicted Answer (after ReAct process): { result . answer } " ) dspy.Retrieve ‚Äã colbertv2_wiki17_abstracts = dspy . ColBERTv2 ( url = 'http://20.102.90.50:2017/wiki17_abstracts' ) dspy . settings . configure ( rm = colbertv2_wiki17_abstracts ) #Define Retrieve Module retriever = dspy . Retrieve ( k = 3 ) query = 'When was the first FIFA World Cup held?' # Call the retriever on a particular query. topK_passages = retriever ( query ) . passages for idx , passage in enumerate ( topK_passages ) : print ( f' { idx + 1 } ]' , passage , '\n' ) DSPy Metrics ‚Äã Function as Metric ‚Äã To create a custom metric you can create a function that returns either a number or a boolean value: def parse_integer_answer ( answer , only_first_line = True ) : try : if only_first_line : answer = answer . strip ( ) . split ( '\n' ) [ 0 ] # find the last token that has a number in it answer = [ token for token in answer . split ( ) if any ( c . isdigit ( ) for c in token ) ] [ - 1 ] answer = answer . split ( '.' ) [ 0 ] answer = '' . join ( [ c for c in answer if c . isdigit ( ) ] ) answer = int ( answer ) except ( ValueError , IndexError ) : # print(answer) answer = 0 return answer # Metric Function def gsm8k_metric ( gold , pred , trace = None ) - > int : return int ( parse_integer_answer ( str ( gold . answer ) ) ) == int ( parse_integer_answer ( str ( pred . answer ) ) ) LLM as Judge ‚Äã class FactJudge ( dspy . Signature ) : """Judge if the answer is factually correct based on the context.""" context = dspy . InputField ( desc = "Context for the prediciton" ) question = dspy . InputField ( desc = "Question to be answered" ) answer = dspy . InputField ( desc = "Answer for the question" ) factually_correct = dspy . OutputField ( desc = "Is the answer factually correct based on the context?" , prefix = "Factual[Yes/No]:" ) judge = dspy . ChainOfThought ( FactJudge ) def factuality_metric ( example , pred ) : factual = judge ( context = example . context , question = example . question , answer = pred . answer ) return int ( factual == "Yes" ) DSPy Evaluation ‚Äã from dspy . evaluate import Evaluate evaluate_program = Evaluate ( devset = devset , metric = your_defined_metric , num_threads = NUM_THREADS , display_progress = True , display_table = num_rows_to_display ) evaluate_program ( your_dspy_program ) DSPy Optimizers ‚Äã LabeledFewShot ‚Äã from dspy . teleprompt import LabeledFewShot labeled_fewshot_optimizer = LabeledFewShot ( k = 8 ) your_dspy_program_compiled = labeled_fewshot_optimizer . compile ( student = your_dspy_program , trainset = trainset ) BootstrapFewShot ‚Äã from dspy . teleprompt import BootstrapFewShot fewshot_optimizer = BootstrapFewShot ( metric = your_defined_metric , max_bootstrapped_demos = 4 , max_labeled_demos = 16 , max_rounds = 1 , max_errors = 5 ) your_dspy_program_compiled = fewshot_optimizer . compile ( student = your_dspy_program , trainset = trainset ) Using another LM for compilation, specifying in teacher_settings ‚Äã from dspy . teleprompt import BootstrapFewShot fewshot_optimizer = BootstrapFewShot ( metric = your_defined_metric , max_bootstrapped_demos = 4 , max_labeled_demos = 16 , max_rounds = 1 , max_errors = 5 , teacher_settings = dict ( lm = gpt4 ) ) your_dspy_program_compiled = fewshot_optimizer . compile ( student = your_dspy_program , trainset = trainset ) Compiling a compiled program - bootstrapping a bootstrapped program ‚Äã your_dspy_program_compiledx2 = teleprompter . compile ( your_dspy_program , teacher = your_dspy_program_compiled , trainset = trainset , ) Saving/loading a compiled program ‚Äã save_path = './v1.json' your_dspy_program_compiledx2 . save ( save_path ) loaded_program = YourProgramClass ( ) loaded_program . load ( path = save_path ) BootstrapFewShotWithRandomSearch ‚Äã from dspy . teleprompt import BootstrapFewShotWithRandomSearch fewshot_optimizer = BootstrapFewShotWithRandomSearch ( metric = your_defined_metric , max_bootstrapped_demos = 2 , num_candidate_programs = 8 , num_threads = NUM_THREADS ) your_dspy_program_compiled = fewshot_optimizer . compile ( student = your_dspy_program , trainset = trainset , valset = devset ) Other custom configurations are similar to customizing the BootstrapFewShot optimizer. Ensemble ‚Äã from dspy . teleprompt import BootstrapFewShotWithRandomSearch from dspy . teleprompt . ensemble import Ensemble fewshot_optimizer = BootstrapFewShotWithRandomSearch ( metric = your_defined_metric , max_bootstrapped_demos = 2 , num_candidate_programs = 8 , num_threads = NUM_THREADS ) your_dspy_program_compiled = fewshot_optimizer . compile ( student = your_dspy_program , trainset = trainset , valset = devset ) ensemble_optimizer = Ensemble ( reduce_fn = dspy . majority ) programs = [ x [ - 1 ] for x in your_dspy_program_compiled . candidate_programs ] your_dspy_program_compiled_ensemble = ensemble_optimizer . compile ( programs [ : 3 ] ) BootstrapFinetune ‚Äã from dspy . teleprompt import BootstrapFewShotWithRandomSearch , BootstrapFinetune #Compile program on current dspy.settings.lm fewshot_optimizer = BootstrapFewShotWithRandomSearch ( metric = your_defined_metric , max_bootstrapped_demos = 2 , num_threads = NUM_THREADS ) your_dspy_program_compiled = tp . compile ( your_dspy_program , trainset = trainset [ : some_num ] , valset = trainset [ some_num : ] ) #Configure model to finetune config = dict ( target = model_to_finetune , epochs = 2 , bf16 = True , bsize = 6 , accumsteps = 2 , lr = 5e-5 ) #Compile program on BootstrapFinetune finetune_optimizer = BootstrapFinetune ( metric = your_defined_metric ) finetune_program = finetune_optimizer . compile ( your_dspy_program , trainset = some_new_dataset_for_finetuning_model , ** config ) finetune_program = your_dspy_program #Load program and activate model's parameters in program before evaluation ckpt_path = "saved_checkpoint_path_from_finetuning" LM = dspy . HFModel ( checkpoint = ckpt_path , model = model_to_finetune ) for p in finetune_program . predictors ( ) : p . lm = LM p . activated = False COPRO ‚Äã from dspy . teleprompt import COPRO eval_kwargs = dict ( num_threads = 16 , display_progress = True , display_table = 0 ) copro_teleprompter = COPRO ( prompt_model = model_to_generate_prompts , metric = your_defined_metric , breadth = num_new_prompts_generated , depth = times_to_generate_prompts , init_temperature = prompt_generation_temperature , verbose = False ) compiled_program_optimized_signature = copro_teleprompter . compile ( your_dspy_program , trainset = trainset , eval_kwargs = eval_kwargs ) MIPRO ‚Äã from dspy . teleprompt import MIPRO teleprompter = MIPRO ( prompt_model = model_to_generate_prompts , task_model = model_that_solves_task , metric = your_defined_metric , num_candidates = num_new_prompts_generated , init_temperature = prompt_generation_temperature ) kwargs = dict ( num_threads = NUM_THREADS , display_progress = True , display_table = 0 ) compiled_program_optimized_bayesian_signature = teleprompter . compile ( your_dspy_program , trainset = trainset , num_trials = 100 , max_bootstrapped_demos = 3 , max_labeled_demos = 5 , eval_kwargs = kwargs ) Signature Optimizer with Types ‚Äã from dspy . teleprompt . signature_opt_typed import optimize_signature from dspy . evaluate . metrics import answer_exact_match from dspy . functional import TypedChainOfThought compiled_program = optimize_signature ( student = TypedChainOfThought ( "question -> answer" ) , evaluator = Evaluate ( devset = devset , metric = answer_exact_match , num_threads = 10 , display_progress = True ) , n_iterations = 50 , ) . program KNNFewShot ‚Äã from dspy . predict import KNN from dspy . teleprompt import KNNFewShot knn_optimizer = KNNFewShot ( KNN , k = 3 , trainset = trainset ) your_dspy_program_compiled = knn_optimizer . compile ( student = your_dspy_program , trainset = trainset , valset = devset ) BootstrapFewShotWithOptuna ‚Äã from dspy . teleprompt import BootstrapFewShotWithOptuna fewshot_optuna_optimizer = BootstrapFewShotWithOptuna ( metric = your_defined_metric , max_bootstrapped_demos = 2 , num_candidate_programs = 8 , num_threads = NUM_THREADS ) your_dspy_program_compiled = fewshot_optuna_optimizer . compile ( student = your_dspy_program , trainset = trainset , valset = devset ) Other custom configurations are similar to customizing the dspy.BootstrapFewShot optimizer. DSPy Assertions ‚Äã Including dspy.Assert and dspy.Suggest statements ‚Äã dspy . Assert ( your_validation_fn ( model_outputs ) , "your feedback message" , target_module = "YourDSPyModuleSignature" ) dspy . Suggest ( your_validation_fn ( model_outputs ) , "your feedback message" , target_module = "YourDSPyModuleSignature" ) Activating DSPy Program with Assertions ‚Äã Note : To use Assertions properly, you must activate a DSPy program that includes dspy.Assert or dspy.Suggest statements from either of the methods above. #1. Using `assert_transform_module: from dspy . primitives . assertions import assert_transform_module , backtrack_handler program_with_assertions = assert_transform_module ( ProgramWithAssertions ( ) , backtrack_handler ) #2. Using `activate_assertions()` program_with_assertions = ProgramWithAssertions ( ) . activate_assertions ( ) Compiling with DSPy Programs with Assertions ‚Äã program_with_assertions = assert_transform_module ( ProgramWithAssertions ( ) , backtrack_handler ) fewshot_optimizer = BootstrapFewShotWithRandomSearch ( metric = your_defined_metric , max_bootstrapped_demos = 2 , num_candidate_programs = 6 ) compiled_dspy_program_with_assertions = fewshot_optimizer . compile ( student = program_with_assertions , teacher = program_with_assertions , trainset = trainset , valset = devset ) #student can also be program_without_assertions Previous FAQs Next Deep Dive DSPy DataLoaders Loading from HuggingFace Datasets Loading specific splits from HuggingFace Loading specific subset from HuggingFace Loading from CSV Splitting a List of dspy.Example Sampling from List of dspy.Example DSPy Programs dspy.Signature dspy.ChainOfThought dspy.ChainOfThoughtwithHint dspy.ProgramOfThought dspy.ReACT dspy.Retrieve DSPy Metrics Function as Metric LLM as Judge DSPy Evaluation DSPy Optimizers LabeledFewShot BootstrapFewShot BootstrapFewShotWithRandomSearch Ensemble BootstrapFinetune COPRO MIPRO Signature Optimizer with Types KNNFewShot BootstrapFewShotWithOptuna DSPy Assertions Including dspy.Assert and dspy.Suggest statements Activating DSPy Program with Assertions Compiling with DSPy Programs with Assertions Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è Quick Start | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub About DSPy Quick Start Installation Minimal Working Example DSPy Building Blocks Tutorials FAQs DSPy Cheatsheet Deep Dive internal Quick Start Quick Start Getting started with DSPy, for building and optimizing LM pipelines. üìÑÔ∏è Installation To install DSPy run: üìÑÔ∏è Minimal Working Example In this post, we walk you through a minimal working example using the DSPy library. Previous About DSPy Next Installation Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è DSPy Building Blocks | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub About DSPy Quick Start DSPy Building Blocks Using DSPy in 8 Steps Language Models Signatures Modules Data Metrics Optimizers (formerly Teleprompters) DSPy Assertions Typed Predictors Tutorials FAQs DSPy Cheatsheet Deep Dive internal DSPy Building Blocks DSPy Building Blocks DSPy introduces signatures (to abstract prompts), modules (to abstract prompting techniques), and optimizers that can tune the prompts (or weights) of modules. üìÑÔ∏è Using DSPy in 8 Steps Using DSPy well for solving a new task is just doing good machine learning with LMs. üìÑÔ∏è Language Models The most powerful features in DSPy revolve around algorithmically optimizing the prompts (or weights) of LMs, especially when you're building programs that use the LMs within a pipeline. üìÑÔ∏è Signatures When we assign tasks to LMs in DSPy, we specify the behavior we need as a Signature. üìÑÔ∏è Modules A DSPy module is a building block for programs that use LMs. üìÑÔ∏è Data DSPy is a machine learning framework, so working in it involves training sets, development sets, and test sets. üìÑÔ∏è Metrics DSPy is a machine learning framework, so you must think about your automatic metrics for evaluation (to track your progress) and optimization (so DSPy can make your programs more effective). üìÑÔ∏è Optimizers (formerly Teleprompters) A DSPy optimizer is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy. üìÑÔ∏è DSPy Assertions Introduction üìÑÔ∏è Typed Predictors In DSPy Signatures, we have InputField and OutputField that define the nature of inputs and outputs of the field. However, the inputs and output to these fields are always str-typed, which requires input and output string processing. Previous Minimal Working Example Next Using DSPy in 8 Steps Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è FAQs | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub About DSPy Quick Start DSPy Building Blocks Tutorials FAQs DSPy Cheatsheet Deep Dive internal FAQs On this page FAQs Is DSPy right for me? DSPy vs. other frameworks ‚Äã The DSPy philosophy and abstraction differ significantly from other libraries and frameworks, so it's usually straightforward to decide when DSPy is (or isn't) the right framework for your usecase. If you're a NLP/AI researcher (or a practitioner exploring new pipelines or new tasks), the answer is generally an invariable yes . If you're a practitioner doing other things, please read on. DSPy vs. thin wrappers for prompts (OpenAI API, MiniChain, basic templating) In other words: Why can't I just write my prompts directly as string templates? Well, for extremely simple settings, this might work just fine. (If you're familiar with neural networks, this is like expressing a tiny two-layer NN as a Python for-loop. It kinda works.) However, when you need higher quality (or manageable cost), then you need to iteratively explore multi-stage decomposition, improved prompting, data bootstrapping, careful finetuning, retrieval augmentation, and/or using smaller (or cheaper, or local) models. The true expressive power of building with foundation models lies in the interactions between these pieces. But every time you change one piece, you likely break (or weaken) multiple other components. DSPy cleanly abstracts away ( and powerfully optimizes) the parts of these interactions that are external to your actual system design. It lets you focus on designing the module-level interactions: the same program expressed in 10 or 20 lines of DSPy can easily be compiled into multi-stage instructions for GPT-4 , detailed prompts for Llama2-13b , or finetunes for T5-base . Oh, and you wouldn't need to maintain long, brittle, model-specific strings at the core of your project anymore. DSPy vs. application development libraries like LangChain, LlamaIndex LangChain and LlamaIndex target high-level application development; they offer batteries-included , pre-built application modules that plug in with your data or configuration. If you'd be happy to use a generic, off-the-shelf prompt for question answering over PDFs or standard text-to-SQL, you will find a rich ecosystem in these libraries. DSPy doesn't internally contain hand-crafted prompts that target specific applications. Instead, DSPy introduces a small set of much more powerful and general-purpose modules that can learn to prompt (or finetune) your LM within your pipeline on your data . when you change your data, make tweaks to your program's control flow, or change your target LM, the DSPy compiler can map your program into a new set of prompts (or finetunes) that are optimized specifically for this pipeline. Because of this, you may find that DSPy obtains the highest quality for your task, with the least effort, provided you're willing to implement (or extend) your own short program. In short, DSPy is for when you need a lightweight but automatically-optimizing programming model ‚Äî not a library of predefined prompts and integrations. If you're familiar with neural networks: This is like the difference between PyTorch (i.e., representing DSPy ) and HuggingFace Transformers (i.e., representing the higher-level libraries). DSPy vs. generation control libraries like Guidance, LMQL, RELM, Outlines These are all exciting new libraries for controlling the individual completions of LMs, e.g., if you want to enforce JSON output schema or constrain sampling to a particular regular expression. This is very useful in many settings, but it's generally focused on low-level, structured control of a single LM call. It doesn't help ensure the JSON (or structured output) you get is going to be correct or useful for your task. In contrast, DSPy automatically optimizes the prompts in your programs to align them with various task needs, which may also include producing valid structured outputs. That said, we are considering allowing Signatures in DSPy to express regex-like constraints that are implemented by these libraries. Basic Usage ‚Äã How should I use DSPy for my task? We wrote a eight-step guide on this. In short, using DSPy is an iterative process. You first define your task and the metrics you want to maximize, and prepare a few example inputs ‚Äî typically without labels (or only with labels for the final outputs, if your metric requires them). Then, you build your pipeline by selecting built-in layers ( modules ) to use, giving each layer a signature (input/output spec), and then calling your modules freely in your Python code. Lastly, you use a DSPy optimizer to compile your code into high-quality instructions, automatic few-shot examples, or updated LM weights for your LM. How do I convert my complex prompt into a DSPy pipeline? See the same answer above. What do DSPy optimizers tune? Or, what does compiling actually do? Each optimizer is different, but they all seek to maximize a metric on your program by updating prompts or LM weights. Current DSPy optimizers can inspect your data, simulate traces through your program to generate good/bad examples of each step, propose or refine instructions for each step based on past results, finetune the weights of your LM on self-generated examples, or combine several of these to improve quality or cut cost. We'd love to merge new optimizers that explore a richer space: most manual steps you currently go through for prompt engineering, "synthetic data" generation, or self-improvement can probably generalized into a DSPy optimizer that acts on arbitrary LM programs. Other FAQs. We welcome PRs to add formal answers to each of these here. You will find the answer in existing issues, tutorials, or the papers for all or most of these. How do I get multiple outputs? You can specify multiple output fields. For the short-form signature, you can list multiple outputs as comma separated values, following the "->" indicator (e.g. "inputs -> output1, output2"). For the long-form signature, you can include multiple dspy.OutputField s. How can I work with long responses? You can specify the generation of long responses as a dspy.OutputField . To ensure comprehensive checks of the content within the long-form generations, you can indicate the inclusion of citations per referenced context. Such constraints such as response length or citation inclusion can be stated through Signature descriptions, or concretely enforced through DSPy Assertions. Check out the LongFormQA notebook to learn more about Generating long-form length responses to answer questions . How can I ensure that DSPy doesn't strip new line characters from my inputs or outputs? DSPy uses Signatures to format prompts passed into LMs. In order to ensure that new line characters aren't stripped from longer inputs, you must specify format=str when creating a field. class UnstrippedSignature ( dspy . Signature ) : """Enter some information for the model here.""" title = dspy . InputField ( ) object = dspy . InputField ( format = str ) result = dspy . OutputField ( format = str ) object can now be a multi-line string without issue. How do I define my own metrics? Can metrics return a float? You can define metrics as simply Python functions that process model generations and evaluate them based on user-defined requirements. Metrics can compare existent data (e.g. gold labels) to model predictions or they can be used to assess various components of an output using validation feedback from LMs (e.g. LLMs-as-Judges). Metrics can return bool , int , and float types scores. Check out the official Metrics docs to learn more about defining custom metrics and advanced evaluations using AI feedback and/or DSPy programs. How expensive or slow is compiling?? To reflect compiling metrics, we highlight an experiment for reference, compiling the SimplifiedBaleen using the dspy.BootstrapFewShotWithRandomSearch optimizer on the gpt-3.5-turbo-1106 model over 7 candidate programs and 10 threads. We report that compiling this program takes around 6 minutes with 3200 API calls, 2.7 million input tokens and 156,000 output tokens, reporting a total cost of $3 USD (at the current pricing of the OpenAI model). Compiling DSPy optimizers naturally will incur additional LM calls, but we substantiate this overhead with minimalistic executions with the goal of maximizing performance. This invites avenues to enhance performance of smaller models by compiling DSPy programs with larger models to learn enhanced behavior during compile-time and propagate such behavior to the tested smaller model during inference-time. Deployment or Reproducibility Concerns ‚Äã How do I save a checkpoint of my compiled program? Here is an example of saving/loading a compiled module: cot_compiled = teleprompter . compile ( CoT ( ) , trainset = trainset , valset = devset ) #Saving cot_compiled . save ( 'compiled_cot_gsm8k.json' ) #Loading: cot = CoT ( ) cot . load ( 'compiled_cot_gsm8k.json' ) How do I export for deployment? Exporting DSPy programs is simply saving them as highlighted above! How do I search my own data? Open source libraries such as RAGautouille enable you to search for your own data through advanced retrieval models like ColBERT with tools to embed and index documents. Feel free to integrate such libraries to create searchable datasets while developing your DSPy programs! How do I turn off the cache? How do I export the cache? You can turn off the cache by setting the DSP_CACHEBOOL environment variable to False , which disables the cache_turn_on flag. Your local cache will be saved to the global env directory os.environ["DSP_CACHEDIR"] or for notebooks os.environ["DSP_NOTEBOOK_CACHEDIR"] . You can usually set the cachedir to os.path.join(repo_path, 'cache') and export this cache from here: os . environ [ "DSP_NOTEBOOK_CACHEDIR" ] = os . path . join ( os . getcwd ( ) , 'cache' ) Advanced Usage ‚Äã How do I parallelize? You can parallelize DSPy programs during both compilation and evaluation by specifying multiple thread settings in the respective DSPy optimizers or within the dspy.Evaluate utility function. How do freeze a module? Modules can be frozen by setting their ._compiled attribute to be True, indicating the module has gone through optimizer compilation and should not have its parameters adjusted. This is handled internally in optimizers such as dspy.BootstrapFewShot where the student program is ensured to be frozen before the teacher propagates the gathered few-shot demonstrations in the bootstrapping process. How do I get JSON output? You can specify JSON-type descriptions in the desc field of the long-form signature dspy.OutputField (e.g. output = dspy.OutputField(desc='key-value pairs') ). If you notice outputs are still not conforming to JSON formatting, try Asserting this constraint! Check out Assertions (or the next question!) How do I use DSPy assertions? a) How to Add Assertions to Your Program : Define Constraints : Use dspy.Assert and/or dspy.Suggest to define constraints within your DSPy program. These are based on boolean validation checks for the outcomes you want to enforce, which can simply be Python functions to validate the model outputs. Integrating Assertions : Keep your Assertion statements following a model generations (hint: following a module layer) b) How to Activate the Assertions : Using assert_transform_module : Wrap your DSPy module with assertions using the assert_transform_module function, along with a backtrack_handler . This function transforms your program to include internal assertions backtracking and retry logic, which can be customized as well: program_with_assertions = assert_transform_module(ProgramWithAssertions(), backtrack_handler) Activate Assertions : Directly call activate_assertions on your DSPy program with assertions: program_with_assertions = ProgramWithAssertions().activate_assertions() Note : To use Assertions properly, you must activate a DSPy program that includes dspy.Assert or dspy.Suggest statements from either of the methods above. Errors ‚Äã How do I deal with "context too long" errors? If you're dealing with "context too long" errors in DSPy, you're likely using DSPy optimizers to include demonstrations within your prompt, and this is exceeding your current context window. Try reducing these parameters (e.g. max_bootstrapped_demos and max_labeled_demos ). Additionally, you can also reduce the number of retrieved passages/docs/embeddings to ensure your prompt is fitting within your model context length. A more general fix is simply increasing the number of max_tokens specified to the LM request (e.g. lm = dspy.OpenAI(model = ..., max_tokens = ... ). How do I deal with timeouts or backoff errors? Firstly, please refer to your LM/RM provider to ensure stable status or sufficient rate limits for your use case! Additionally, try reducing the number of threads you are testing on as the corresponding servers may get overloaded with requests and trigger a backoff + retry mechanism. If all variables seem stable, you may be experiencing timeouts or backoff errors due to incorrect payload requests sent to the api providers. Please verify your arguments are compatible with the SDK you are interacting with. At times, DSPy may have hard-coded arguments that are not relevant for your compatible, in which case, please free to open a PR alerting this or comment out these default settings for your usage. Contributing ‚Äã What if I have a better idea for prompting or synthetic data generation? Perfect. We encourage you to think if it's best expressed as a module or an optimizer, and we'd love to merge it in DSPy so everyone can use it. DSPy is not a complete project; it's an ongoing effort to create structure (modules and optimizers) in place of hacky prompt and pipeline engineering tricks. How can I add my favorite LM or vector store? Check out these walkthroughs on setting up a Custom LM client and Custom RM client . Previous Additional Resources Next DSPy Cheatsheet Is DSPy right for me? DSPy vs. other frameworks Basic Usage Deployment or Reproducibility Concerns Advanced Usage Errors Contributing Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è Deep Dive | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub About DSPy Quick Start DSPy Building Blocks Tutorials FAQs DSPy Cheatsheet Deep Dive Data Handling Signatures Modules Typed Predictors Language Model Clients Retrieval Model Clients Teleprompters internal Deep Dive Deep Dive DSPy introduces signatures (to abstract prompts), modules (to abstract prompting techniques), and optimizers that can tune the prompts (or weights) of modules. üóÉÔ∏è Data Handling 3 items üóÉÔ∏è Signatures 3 items üóÉÔ∏è Modules 6 items üóÉÔ∏è Typed Predictors 2 items üóÉÔ∏è Language Model Clients 3 items üóÉÔ∏è Retrieval Model Clients 6 items üóÉÔ∏è Teleprompters 2 items Previous DSPy Cheatsheet Next Data Handling Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è Build & Release Workflow Implementation | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub About DSPy Quick Start DSPy Building Blocks Tutorials FAQs DSPy Cheatsheet Deep Dive internal Build & Release Workflow Implementation Release Checklist internal Build & Release Workflow Implementation On this page Build & Release Workflow Implementation The build_and_release workflow automates deployments of dspy-ai to pypi. For a guide to triggering a release using the workflow, refer to release checklist . Overview ‚Äã At a high level, the workflow works as follows: Maintainer of the repo pushes a tag following semver versioning for the new release. This triggers the github action which extracts the tag (the version) Builds and publishes a release on test-pypi Uses the test-pypi release to run build_utils/tests/intro.py with the new release as an integration test. Note intro.py is a copy of the intro notebook. Assuming the test runs successfully, it pushes a release to pypi . If not, the user can delete the tag, make the fixes and then push the tag again. Versioning for multiple releases to test-pypi with the same tag version is taken care of by the workflow by appending a pre-release identifier, so the user only needs to consider the version for pypi. (Currently manual) the user creates a release and includes release notes, as described in docs/docs/release-checklist.md Implementation Details ‚Äã The workflow executes a series of jobs in sequence: extract-tag build-and-publish-test-pypi test-intro-script build-and-publish-pypi extract-tag ‚Äã Extracts the tag pushed to the commit. This tag is expected to be the version of the new deployment. build-and-publish-test-pypi ‚Äã Builds and publishes the package to test-pypi. Determines the version that should be deployed to test-pypi. There may be an existing deployment with the version specified by the tag in the case that a deployment failed and the maintainer made some changes and pushed the same tag again (which is the intended usage). The following logic is implemented test_version.py Load the releases on test-pypi Check if there is a release matching our current tag If not, create a release with the current tag If it exists, oad the latest published version (this will either be the version with the tag itself, or the tag + a pre-release version). In either case, increment the pre-release version. Updates the version placeholder in setup.py to the version obtained in step 1. Updates the version placeholder in pyproject.toml to the version obtained in step 1. Updates the package name placeholder in setup.py to dspy-ai-test * Updates the package name placeholder in pyproject.toml to dspy-ai-test * Builds the binary wheel Publishes the package to test-pypi. test-intro-script ‚Äã Runs the pytest containing the intro script as an integration test using the package published to test-pypi. This is a validation step before publishing to pypi. Uses a loop to install the version just published to test-pypi as sometimes there is a race condition between the package becoming available for installation and this job executing. Runs the test to ensure the package is working as expected. If this fails, the workflow fails and the maintainer needs to make a fix and delete and then recreate the tag. build-and-publish-pypi ‚Äã Builds and publishes the package to pypi. Updates the version placeholder in setup.py to the version obtained in step 1. Updates the version placeholder in pyproject.toml to the version obtained in step 1. Updates the package name placeholder in setup.py to dspy-ai * Updates the package name placeholder in pyproject.toml to dspy-ai * Builds the binary wheel Publishes the package to pypi. * The package name is updated by the worfklow to allow the same files to be used to build both the pypi and test-pypi packages. Previous Signature Optimizer Next Release Checklist Overview Implementation Details Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è About DSPy | DSPy Skip to main content DSPy Documentation Tutorials API References DSPy Cheatsheet GitHub About DSPy Quick Start DSPy Building Blocks Tutorials FAQs DSPy Cheatsheet Deep Dive internal About DSPy On this page About DSPy DSPy is a framework for algorithmically optimizing LM prompts and weights , especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system without DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate synthetic examples to tune each step, and (5) use these examples to finetune smaller LMs to cut costs. Currently, this is hard and messy: every time you change your pipeline, your LM, or your data, all prompts (or finetuning steps) may need to change. To make this more systematic and much more powerful, DSPy does two things. First, it separates the flow of your program ( modules ) from the parameters (LM prompts and weights) of each step. Second, DSPy introduces new optimizers , which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a metric you want to maximize. DSPy can routinely teach powerful models like GPT-3.5 or GPT-4 and local models like T5-base or Llama2-13b to be much more reliable at tasks, i.e. having higher quality and/or avoiding specific failure patterns. DSPy optimizers will "compile" the same program into different instructions, few-shot prompts, and/or weight updates (finetunes) for each LM. This is a new paradigm in which LMs and their prompts fade into the background as optimizable pieces of a larger system that can learn from data. tldr; less prompting, higher scores, and a more systematic approach to solving hard tasks with LMs. Analogy to Neural Networks ‚Äã When we build neural networks, we don't write manual for-loops over lists of hand-tuned floats. Instead, you might use a framework like PyTorch to compose layers (e.g., Convolution or Dropout ) and then use optimizers (e.g., SGD or Adam) to learn the parameters of the network. Ditto! DSPy gives you the right general-purpose modules (e.g., ChainOfThought , ReAct , etc.), which replace string-based prompting tricks. To replace prompt hacking and one-off synthetic data generators, DSPy also gives you general optimizers ( BootstrapFewShotWithRandomSearch or MIPRO ), which are algorithms that update parameters in your program. Whenever you modify your code, your data, your assertions, or your metric, you can compile your program again and DSPy will create new effective prompts that fit your changes. Next Quick Start Analogy to Neural Networks Docs Documentation API Reference Community See all 130+ contributors on GitHub More GitHub Built with ‚å®Ô∏è